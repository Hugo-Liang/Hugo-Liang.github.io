@article{2025-EXSY-SETCS,
author = {Liang, Yu-Guo and Fan, Gui-Sheng and Yu, Hui-Qun and Li, Ming-Chen and Huang, Zi-Jie},
title = {Automatic Code Summarization Using Abbreviation Expansion and Subword Segmentation},
journal = {Expert Systems},
volume = {42},
number = {2},
pages = {e13835},
keywords = {automatic code summarization, code abbreviation expansion, deep learning, program understanding, subword segmentation},
doi = {https://doi.org/10.1111/exsy.13835},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.13835},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/exsy.13835},
note = {e13835 EXSY-Jun-24-2582.R1},
abstract = {ABSTRACT Automatic code summarization refers to generating concise natural language descriptions for code snippets. It is vital for improving the efficiency of program understanding among software developers and maintainers. Despite the impressive strides made by deep learning-based methods, limitations still exist in their ability to understand and model semantic information due to the unique nature of programming languages. We propose two methods to boost code summarization models: context-based abbreviation expansion and unigram language model-based subword segmentation. We use heuristics to expand abbreviations within identifiers, reducing semantic ambiguity and improving the language alignment of code summarization models. Furthermore, we leverage subword segmentation to tokenize code into finer subword sequences, providing more semantic information during training and inference, thereby enhancing program understanding. These methods are model-agnostic and can be readily integrated into existing automatic code summarization approaches. Experiments conducted on two widely used Java code summarization datasets demonstrated the effectiveness of our approach. Specifically, by fusing original and modified code representations into the Transformer model, our Semantic Enhanced Transformer for Code Summarizsation (SETCS) serves as a robust semantic-level baseline. By simply modifying the datasets, our methods achieved performance improvements of up to 7.3\%, 10.0\%, 6.7\%, and 3.2\% for representative code summarization models in terms of BLEU-4, METEOR, ROUGE-L and SIDE, respectively.},
year = {2025}
}

