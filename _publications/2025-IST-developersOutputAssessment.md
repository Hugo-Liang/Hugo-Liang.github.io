---
title: "Usage patterns of software product metrics in assessing developers’ output: A comprehensive study"
collection: manuscripts
permalink: /publication/2025-IST-developersOutputAssessment
excerpt: '• We explore how developers can intentionally manipulate the LOC metric using LLMs, leading to significant anomalies that affect the fairness and effectiveness of developers’ output assessments.<br> • We provide a thorough evaluation of existing product metrics, with a particular focus on efficiency metrics and quality metrics from SATs, assessing their practicality and cost-effectiveness.<br> • We conduct a rapid review of quantitative metrics used in past developers’ output research to help the company select relevant software metrics, providing guidance for future quantitative assessments of developers’ output.<br> • We establish a connection between academic research on software product metrics and practical applications in the industry, demonstrating how academic insights can influence real-world developers’ output assessment practices.'
date: 2025-10-17
venue: 'Information and Software Technology'
paperurl: 'http://hugo-liang.github.io/files/2025-IST-developersOutputAssessment.pdf'
bibtexurl: 'http://hugo-liang.github.io/files/2025-IST-developersOutputAssessment.bib'
citation: 'Wentao Chen, Huiqun Yu<sup>*</sup>, Guisheng Fan<sup>*</sup>, Zijie Huang, <strong>Yuguo Liang</strong>. Usage Patterns of Software Product Metrics in Assessing Developers&apos; Output: A Comprehensive Study. Information and Software Technology, 2025, 107935. <a href=&quot;https://doi.org/10.1016/j.infsof.2025.107935&quot;>https://doi.org/10.1016/j.infsof.2025.107935</a>. [CCF-B / SCI-Q2]'
---Context:<br> Accurate assessment of developers’ output is crucial for both software engineering research and industrial practice. This assessment often relies on software product metrics such as lines of code (LOC) and quality indicators from static analysis tools. However, existing research lacks a comprehensive understanding of the usage patterns of product metrics, and a single metric is increasingly vulnerable to manipulation, particularly with the emergence of large language models (LLMs).<br> Objectives: <br> This study aims to investigate (1) how developers can intentionally manipulate commonly used metrics like LOC by using LLMs, (2) whether complex efficiency metrics provide consistent advantages over simpler metrics, and (3) the reliability and cost-effectiveness of quality metrics derived from tools such as SonarQube.<br> Methods:<br> We conduct empirical analyses involving three LLMs to achieve metric manipulation and evaluate product metric reliability across nine open-source projects. We further validate our findings through a collaboration with a large financial institution facing fairness concerns in developers’ output due to inappropriate metric usage. <br> Results:<br> We observe that developers can inflate LOC by an average of 60.86% using LLMs, leading to anomalous assessments. Complex efficiency metrics do not yield consistent performance improvements relative to their computational costs. Furthermore, quality metrics from SonarQube and PMD often fail to capture real quality changes and are expensive to compute. The software metric migration plan based on our findings effectively reduces evaluation anomalies in the industry and standardizes developers’ commits, confirming our conclusions’ practical validity.<br> Conclusion:<br> Our findings highlight critical limitations in current metric practices and demonstrate how thoughtful usage patterns of product metrics can improve fairness in developer evaluation. This work bridges the gap between academic insights and industrial needs, offering practical guidance for more reliable developers’ output assessment.